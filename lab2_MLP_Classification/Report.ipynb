{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0081d975-63e0-49e6-b0d7-a597288d6423",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c7c81-20d8-48f8-85ce-e25d78cf81f1",
   "metadata": {},
   "source": [
    "**Name:** Erik Lidbeck\n",
    "**Date:** 2021-01-28\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This lab introduces \n",
    "\n",
    "## Result\n",
    "\n",
    "### Exercise 1\n",
    "A single hidden node wasn't enough to seperate the two classes with 100 samples\n",
    "\n",
    "### Exercise 2\n",
    "#### Task 1\n",
    "I managed to get 88.1 % Accuracy\n",
    "#### Task 2\n",
    "Accuracy: 95%\n",
    "Batch size: 50\n",
    "Epochs: 1000\n",
    "Hidden nodes: 6000\n",
    "Learning rate: 0.00005\n",
    "\n",
    "Task 2 overfitted model ran for longer epochs and had a lower validation loss but if compared at the ending step of the linear model in Task 1 then the overfitted model had an increase of 0.1493 validation loss\n",
    "\n",
    "The optimal number of hidden nodes to maximize validation performance is 300 hidden nodes.\n",
    "\n",
    "<center><img src=\"./loss_val.png\" width=\"600\"></center>\n",
    "\n",
    "#### Task 3\n",
    "With 2000 hidden nodes I was able to get 83% validation accuracy which I'd consider a reasonable result. More than 4900 hidden nodes didn't get a boundary.\n",
    "\n",
    "### Exercise 3\n",
    "#### Task 1\n",
    "I'm using loss from tensorboard because stats_reg isn't compatible with nn.Module \n",
    "\n",
    "In order of the list: Version 0 = 5 hidden nodes, 1 = 10, 2 = 20, 3 = 30, 4 = 50, 5 = 100, 6 = 150, 7 = 300, 8 = 400\n",
    "<center><img src=\"./loss_train_and_val_t1.png\" width=\"600\"></center>\n",
    "Here the loss seem to grow greater in training than validation when increasing complexity. This is because there isn't any noise to overtrain the model to.\n",
    "\n",
    "#### Task 2\n",
    "Using the same increment as before starting with version 10 = 5, and so on...\n",
    "<center><img src=\"./loss_t2.png\" width=\"600\"></center>\n",
    "The training loss has overall decrease compared to previously and this is because now there is injected noise for the model to learn. From the graph it looks to be version 15 (100 hidden nodes) that is optimal in validation with a loss at 0.6445. \n",
    "\n",
    "#### Task 3\n",
    "Instead of increasing number of hidden nodes we now implement l2 regularization by using weight decay in the optimizer.\n",
    "In order of the list: Version 19 = 0.000001, 20 = 0.000005, 21 = 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1. \n",
    "<center><img src=\"./loss_l2_t3.png\" width=\"600\"></center>\n",
    "By using this method we find that Version 25 (decay = 0.001) get the optimal validation performance  0.682\n",
    "\n",
    "#### Task 4\n",
    "Even though I did not get a lower loss with regularization than using different numbers of hidden nodes, it certainly did have a noticeable effect. It should be possible to increase the performance further by finetuning other parameters such as batch size.\n",
    "\n",
    "### Exercise 4\n",
    "#### Task 1\n",
    "After experimenting with the amount of layers (using a between 15-30 hidden nodes) I quickly saw that more layers often made the model words and sometimes would yield results like this:\n",
    "<center><img src=\"./Interresting.png\" width=\"400\"></center>\n",
    "So I abandoned trying a high number of layers and focused on the number of hidden nodes. After trying a range of hidden nodes [10, 20, 30, 40, 50, 60, 70, 80, 90, 100] I couldn't seem to decrease the loss substantially. I began to look back at model itself and change the ouput activation. At first I was trying ReLu and switched to SoftMax which didn't have any noticable impact, then I changed to a simple linear activation. This resulted in a drastic decrease in loss. A model that had 1.81 loss and 74% accuracy became 0.32 loss and 90% accuracy.\n",
    "<center><img src=\"./linear.png\" width=\"800\"></center>\n",
    "\n",
    "After testing trying different learning rates [0.001, 0.002, 0.004, 0.006, 0.008, 0.01] to move out of plateaus (seen in the red and blue)\n",
    "<center><img src=\"./2_4_task1.png\" width=\"800\"></center>\n",
    "\n",
    "the best model was 80 Hidden nodes, 1 Layer, lr = 0.004, \n",
    "\n",
    "#### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780f80b",
   "metadata": {},
   "source": [
    "Now I will classify a spiral dataset. This task is a binary classification problem, and therefore I will use binary cross entropy loss. The aim of this task is to make a model that correctly classifies all training samples. I will use a max_epoch = 1000.\n",
    "\n",
    "                                                             The data\n",
    "<center><img src=\"./0204_data.png\" width=\"400\"></center>\n",
    "\n",
    "From experimenting with different combinations of nodes and layers I noticed that wider networks worked best (similar to previous tasks). I was able to get perfect classification with this setup:\n",
    "\n",
    "                    1 hidden layer x 100 hidden nodes, learning rate = 0.005, batch size = 25\n",
    "\n",
    "<center><img src=\"./0204_wide.png\" width=\"800\"></center>\n",
    "\n",
    "But I wasn't happy with this result. How small can I make it? At first I tried halving the hidden nodes and changing the learning rate. I was able to get the best result with a learning rate of 0.011\n",
    "\n",
    "<center><img src=\"./0204_smaller.png\" width=\"800\"></center>\n",
    "\n",
    "The uptick visible shows that because of the high learning rate the optimizer might jump to a higher loss. This could mean that a slightly lower learning rate would be beneficient. But after trying 0.009 and 0.010 three times each, both would reach perfect classification 1/3 while 0.011 would always reach perfect classification (the rest would be around 95% accurate). I must admit that this sample size is too small to accuratly measure the accuracy of the settings, but it does provide a small insight.\n",
    "\n",
    "Adding more layer with less hidden nodes doesn't increase the performance no matter learning rate. The lowest total of hidden nodes I could get was 33.\n",
    "\n",
    "<center><img src=\"./0204_smallest.png\" width=\"400\"></center>\n",
    "\n",
    "To be fair it's not always accurate after every training so a little luck is needed.\n",
    "<center><img src=\"./0204_33.png\" width=\"600\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
