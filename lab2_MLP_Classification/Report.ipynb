{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0081d975-63e0-49e6-b0d7-a597288d6423",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c7c81-20d8-48f8-85ce-e25d78cf81f1",
   "metadata": {},
   "source": [
    "**Name:** Erik Lidbeck\n",
    "**Date:** 2021-12-06\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Result\n",
    "\n",
    "### Exercise 1\n",
    "A single hidden node wasn't enough to seperate the two classes with 100 samples\n",
    "\n",
    "### Exercise 2\n",
    "#### Task 1\n",
    "88.1 % Accuracy\n",
    "#### Task 2\n",
    "Accuracy: 95%\n",
    "Batch size: 50\n",
    "Epochs: 1000\n",
    "Hidden nodes: 6000\n",
    "Learning rate: 0.00005\n",
    "\n",
    "Task 2 overfitted model ran for longer epochs and had a lower validation loss but if compared at the ending step of the linear model in Task 1 then the overfitted model had an increase of 0.1493 validation loss\n",
    "\n",
    "The optimal number of hidden nodes to maximize validation performance is 300 hidden nodes.\n",
    "\n",
    "<center><img src=\"./loss_val.png\" width=\"600\"></center>\n",
    "\n",
    "#### Task 3\n",
    "With 2000 hidden nodes I was able to get 83% validation accuracy which I'd consider a reasonable result. More than 4900 hidden nodes didn't get a boundary.\n",
    "\n",
    "### Exercise 3\n",
    "#### Task 1\n",
    "I'm using loss from tensorboard because stats_reg isn't compatible with nn.Module \n",
    "\n",
    "In order of the list: Version 0 = 5 hidden nodes, 1 = 10, 2 = 20, 3 = 30, 4 = 50, 5 = 100, 6 = 150, 7 = 300, 8 = 400\n",
    "<center><img src=\"./loss_train_and_val_t1.png\" width=\"600\"></center>\n",
    "Here the loss seem to grow greater in training than validation when increasing complexity. This is because there isn't any noise to overtrain the model to.\n",
    "\n",
    "#### Task 2\n",
    "Using the same increment as before starting with version 10 = 5, and so on...\n",
    "<center><img src=\"./loss_t2.png\" width=\"600\"></center>\n",
    "The training loss has overall decrease compared to previously and this is because now there is injected noise for the model to learn. From the graph it looks to be version 15 (100 hidden nodes) that is optimal in validation with a loss at 0.6445. \n",
    "\n",
    "#### Task 3\n",
    "Instead of increasing number of hidden nodes we now implement l2 regularization by using weight decay in the optimizer.\n",
    "In order of the list: Version 19 = 0.000001, 20 = 0.000005, 21 = 0.00001, ..., 29 = 0.1\n",
    "<center><img src=\"./loss_l2_t3.png\" width=\"600\"></center>\n",
    "By using this method we find that Version 25 (decay = 0.001) get the optimal validation performance  0.682\n",
    "\n",
    "#### Task 4\n",
    "\n",
    "### Exercise 4\n",
    "#### Task 1\n",
    "#### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d55ffd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
