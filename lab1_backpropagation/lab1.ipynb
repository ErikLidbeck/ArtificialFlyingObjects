{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise I:<br> Backpropagation\n",
    "</center></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DataH5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import flying_objects_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.DataH5 at 0x7f60702d6250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = flying_objects_config()\n",
    "img_shape = cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH, cfg.IMAGE_CHANNEL\n",
    "\n",
    "DataH5(img_shape, cfg.CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial from pytorch: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "* What is torch, torchvision?\n",
    "* What is a tensor?\n",
    "* How does the neural network work?\n",
    "- Define optimizer\n",
    "- Define loss\n",
    "\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- gradient descent (step)\n",
    "\n",
    "# Common pitfalls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnable parameters?\n",
    "\n",
    "\"\"\"'\n",
    "# Settings\n",
    "- Learning rate?\n",
    "- Classes?\n",
    "- Number of files?\n",
    "\n",
    "\"\"\"\n",
    "num_epochs = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9427b6e0040c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Get input and target. Contains a minibatch!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get input and target. Contains a minibatch!\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward propagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Data (#3)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Defining synthetic data sets\n",
    "\n",
    "This cell defines the three different synthetic data sets and the regression dataset. It also provides functions for reading the Vowles dataset and the Spiral data. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn1(N):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "    \n",
    "    global seed\n",
    "    \n",
    "    data = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    tar = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(N/2)\n",
    "  \n",
    "    data[:N1,0] = 4 + np.random.normal(loc=.0, scale=1., size=(N1))\n",
    "    data[N1:,0] = -4 + np.random.normal(loc=.0, scale=1., size=(N-N1))\n",
    "    data[:,1] = 10*np.random.normal(loc=.0, scale=1., size=(N))\n",
    "    \n",
    "    \n",
    "    data = data / data.std(axis=0)\n",
    "    \n",
    "    # Target\n",
    "    tar[:N1] = np.ones(shape=(N1,))\n",
    "    tar[N1:] = np.zeros(shape=(N-N1,))\n",
    "    \n",
    "    # Rotation\n",
    "    theta = np.radians(30)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array([[c,-s],[s,c]]) # rotation matrix\n",
    "    data = np.dot(data,R) \n",
    "    \n",
    "    return data,tar\n",
    "\n",
    "\n",
    "def syn2(N):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "    \n",
    "    global seed\n",
    "     \n",
    "    data = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    tar = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(N/2)\n",
    "\n",
    "    # Positive samples\n",
    "    data[:N1,:] = 0.8 + np.random.normal(loc=.0, scale=1., size=(N1,2))\n",
    "    # Negative samples \n",
    "    data[N1:,:] = -.8 + np.random.normal(loc=.0, scale=1., size=(N-N1,2))\n",
    "    \n",
    "    \n",
    "    # Target\n",
    "    tar[:N1] = np.ones(shape=(N1,))\n",
    "    tar[N1:] = np.zeros(shape=(N-N1,))\n",
    "\n",
    "    return data,tar\n",
    "\n",
    "\n",
    "def syn3(N):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "\n",
    "    global seed\n",
    "    \n",
    "    data = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    tar = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(2*N/3)\n",
    "    \n",
    "    # disk\n",
    "    teta_d = np.random.uniform(0, 2*np.pi, N1)\n",
    "    inner, outer = 2, 5\n",
    "    r2 = np.sqrt(np.random.uniform(inner**2, outer**2, N1))\n",
    "    data[:N1,0],data[:N1,1] = r2*np.cos(teta_d), r2*np.sin(teta_d)\n",
    "        \n",
    "    #circle\n",
    "    teta_c = np.random.uniform(0, 2*np.pi, N-N1)\n",
    "    inner, outer = 0, 3\n",
    "    r2 = np.sqrt(np.random.uniform(inner**2, outer**2, N-N1))\n",
    "    data[N1:,0],data[N1:,1] = r2*np.cos(teta_c), r2*np.sin(teta_c)\n",
    "    \n",
    "    # Normalization\n",
    "    #data = data - data.mean(axis=0)/data.std(axis=0)\n",
    "\n",
    "    tar[:N1] = np.ones(shape=(N1,))\n",
    "    tar[N1:] = np.zeros(shape=(N-N1,))\n",
    "    \n",
    "    return data, tar\n",
    "\n",
    "\n",
    "def regr1(N, v=0):\n",
    "    \"\"\" data(samples, features)\"\"\"\n",
    "\n",
    "    global seed\n",
    "\n",
    "    data = np.empty(shape=(N,6), dtype = np.float32)  \n",
    "    \n",
    "    uni = lambda n : np.random.uniform(0,1,n)\n",
    "    norm = lambda n : np.random.normal(0,1,n)\n",
    "    noise =  lambda  n : np.random.normal(0,1,n)\n",
    "    \n",
    "    \n",
    "    for i in range(4):\n",
    "        data[:,i] = norm(N)\n",
    "    for j in [4,5]:\n",
    "        data[:,j] = uni(N)\n",
    "    \n",
    "    tar =   2*data[:,0] + data[:,1]* data[:,2]**2 + np.exp(data[:,3]) + \\\n",
    "            5*data[:,4]*data[:,5]  + 3*np.sin(2*np.pi*data[:,5])\n",
    "    std_signal = np.std(tar)\n",
    "    tar = tar + v * std_signal * noise(N)\n",
    "        \n",
    "    return data, tar\n",
    "\n",
    "def spiral():\n",
    "    tmp = np.loadtxt(\"spiral.dat\")\n",
    "    data, tar = tmp[:, :2], tmp[:, 2]\n",
    "\n",
    "    return data, tar\n",
    "\n",
    "def vowels():\n",
    "    \n",
    "    def pre_proc(file_name):\n",
    "        block = []\n",
    "        x = []\n",
    "    \n",
    "        with open(file_name) as file:\n",
    "            for line in file:    \n",
    "                if line.strip():\n",
    "                    numbers = [float(n) for n in line.split()]\n",
    "                    block.append(numbers)\n",
    "                else:\n",
    "                    x.append(block)\n",
    "                    block = []\n",
    "                \n",
    "        ################################\n",
    "        x = [np.asarray(ar) for ar in x]    \n",
    "        return x\n",
    "\n",
    "    x_train = pre_proc('ae.train')\n",
    "    x_test = pre_proc('ae.test')\n",
    "\n",
    "    \n",
    "    ############## LABELS###########\n",
    "    chunk1 = list(range(30,270, 30))\n",
    "    y_train = []\n",
    "    person = 0\n",
    "\n",
    "    for i, block in enumerate(x_train):\n",
    "        if i in chunk1:\n",
    "            person += 1\n",
    "        y_train.extend([person]*block.shape[0])\n",
    "        \n",
    "    chunk2 = [31,35,88,44,29,24,40,50,29]\n",
    "    chunk2 = np.cumsum(chunk2)\n",
    "    y_test = []\n",
    "    person = 0\n",
    "    for i, block in enumerate(x_test):\n",
    "        if i in chunk2:\n",
    "            person += 1\n",
    "        y_test.extend([person]*block.shape[0])\n",
    "\n",
    "    x_train = np.vstack(x_train)\n",
    "    x_test = np.vstack(x_test)\n",
    "    \n",
    "    ## Split into train, validation and test\n",
    "    num_classes = 9\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.4, random_state=42)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: PlotData (#4)\n",
    "### CellType: Information\n",
    "### Cell instruction: Plotting the data\n",
    "\n",
    "Here we just generate 100 cases for syn1-syn3 and the spiral dataset and plot them. Run the cell by entering into the cell and press \"CTRL Enter\". \n",
    "\n",
    "**Note!** This cell is not needed for the actual exercises, it is just to visualize the four different 2D synthetic classification data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "d,t = syn1(100)\n",
    "plt.figure(1)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n",
    "\n",
    "d,t = syn2(100)\n",
    "plt.figure(2)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n",
    "\n",
    "d,t = syn3(100)\n",
    "plt.figure(3)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n",
    "\n",
    "d,t = spiral()\n",
    "plt.figure(4)\n",
    "plt.scatter(d[:,0],d[:,1], c=t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Statistics (#5)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Present result for both classification and regression problems\n",
    "\n",
    "This cell defines two functions that we are going to call using a trained model to calculate both error and performance measures. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_class(x = None, y = None, label = 'Training', modl = None):\n",
    "    \"\"\"\n",
    "    input :  \n",
    "             x = input\n",
    "             y = output\n",
    "             label = \"Provided text string\"\n",
    "             modl = the model\n",
    "             \n",
    "    output : \n",
    "             sensitivity = fraction of correctly classified positive cases\n",
    "             specificity = fraction of correctly classified negative cases\n",
    "             accuracy = fraction of correctly classified cases\n",
    "             loss = typically the cross-entropy error\n",
    "    \"\"\"\n",
    "    \n",
    "    def binary(y1):\n",
    "        y1[y1>.5] = 1.\n",
    "        y1[y1<= .5] = 0.        \n",
    "        return y1\n",
    "\n",
    "    y_pr = modl.predict(x, batch_size = x.shape[0], verbose=0).reshape(y.shape)\n",
    "                \n",
    "    nof_p, tp, nof_n, tn = [np.count_nonzero(k) for k in [y==1, y_pr[y==1.] > 0.5, y==0, y_pr[y==0.]<= 0.5]]\n",
    "    \n",
    "    sens = tp / nof_p\n",
    "    spec = tn / nof_n\n",
    "    acc = (tp + tn) / (len(y))\n",
    "    loss = modl.evaluate(x, y , batch_size =  x.shape[0], verbose=0)\n",
    "                \n",
    "    A = ['Accuracy', 'Sensitivity', 'Specificity', 'Loss']\n",
    "    B = [acc, sens, spec, loss]\n",
    "    \n",
    "    print('\\n','#'*10,'STATISTICS for {} Data'.format(label), '#'*10, '\\n')\n",
    "    for r in zip(A,B):\n",
    "         print(*r, sep = '   ')\n",
    "    return print('\\n','#'*50)\n",
    "\n",
    "def stats_reg(d = None, d_pred = None, label = 'Training', estimat = None):\n",
    "    \n",
    "    A = ['MSE', 'CorrCoeff']\n",
    "    \n",
    "    pcorr = np.corrcoef(d, d_pred)[1,0]\n",
    "    \n",
    "    if label.lower() in ['training', 'trn', 'train']:\n",
    "        mse = estimat.history['loss'][-1]\n",
    "    else:\n",
    "        mse = estimat.history['val_loss'][-1] \n",
    "\n",
    "    B = [mse, pcorr]\n",
    "    \n",
    "    print('\\n','#'*10,'STATISTICS for {} Data'.format(label), '#'*10, '\\n')\n",
    "    for r in zip(A,B):\n",
    "         print(*r, sep = '   ')\n",
    "    return print('\\n','#'*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Confusion (#6)\n",
    "### CellType: Needed\n",
    "### Cell Instruction: Plot the confusion matrix\n",
    "\n",
    "This cell defines the function that plots the confusion matrix. A confusion matrix is a summary of the predictions made by a classifier. Each column of the matrix represents the instances of the predicted class while each row represents the instances of the actual class. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Boundary (#7)\n",
    "### CellType: Needed\n",
    "### Cell Instruction: Decision boundary\n",
    "\n",
    "This cell defines the function to plot the decision boundary for a 2D input binary MLP classifier. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_b(X = None, Y1 = None ):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    # grid stepsize\n",
    "    h = 0.025\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    Z[Z>.5] = 1\n",
    "    Z[Z<= .5] = 0\n",
    "\n",
    "    Y_pr = model.predict(X, batch_size = X.shape[0], verbose=0).reshape(Y1.shape)\n",
    "  \n",
    "    Y = np.copy(Y1)\n",
    "    Y_pr[Y_pr>.5] = 1\n",
    "    Y_pr[Y_pr<= .5] = 0\n",
    "    Y[(Y!=Y_pr) & (Y==0)] = 2\n",
    "    Y[(Y!=Y_pr) & (Y==1)] = 3\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    #plt.contourf(xx, yy, Z, cmap=plt.cm.PRGn, alpha = .9) \n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    \n",
    "    plt.scatter(X[:, 0][Y==1], X[:, 1][Y==1], marker='+', c='k')\n",
    "    plt.scatter(X[:, 0][Y==0], X[:, 1][Y==0], marker='o', c='k')\n",
    "       \n",
    "    plt.scatter(X[:, 0][Y==3], X[:, 1][Y==3], marker = '+', c='r')   \n",
    "    plt.scatter(X[:, 0][Y==2], X[:, 1][Y==2], marker = 'o', c='r')\n",
    "    \n",
    "    \n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "End of \"Needed\" and \"Information\" cells. Below are the cells for the actual exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex1 (#8)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 1\n",
    "\n",
    "The cell below should be used for question 1. You can run the cell as it is (i.e. CTRL-Return). However, looking at the code will help you understand how the network is created, trained and evaluated. It will be useful for the other questions.\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "Use synthetic data 1 (syn1) (100 data points) and train a linear MLP to separate the two classes, i.e. use a single hidden node. **Why can you solve this problem with a single hidden node?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "    \n",
    "# Generate training data\n",
    "x_train, d_train = syn1(100)\n",
    "\n",
    "# Define the network, cost function and minimization method\n",
    "INPUT = {'inp_dim': x_train.shape[1],         \n",
    "         'n_nod': [1],                      # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'sigmoid',          # output activation function\n",
    "         'opt_method': 'SGD',               # minimization method\n",
    "         'cost_fun': 'binary_crossentropy', # error function\n",
    "         'lr_rate': 0.1,                    # learningrate\n",
    "         'num_out' : 1 }              # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "\n",
    "# Get the model\n",
    "model = pipline(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator = model.fit(x_train, d_train,\n",
    "                      epochs = 300,                     # Number of epochs\n",
    "                      #validation_data=(x_val, y_val),  # We don't have any validation dataset!\n",
    "                      batch_size = x_train.shape[0],    # Use batch learning\n",
    "                      #batch_size=25,                   \n",
    "                      verbose = 0)\n",
    "\n",
    "# Call the stats function to print out statistics for the training\n",
    "stats_class(x_train, d_train, 'Training', model)\n",
    "\n",
    "# Some plotting\n",
    "plt.plot(estimator.history['loss'])\n",
    "plt.title('Model training')\n",
    "plt.ylabel('training error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "# Show the decision boundary\n",
    "decision_b(x_train, d_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex2 (#9)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 2-5\n",
    "\n",
    "The cell below should be used for questions 2-5. For question 2 you can run the cell as it is (i.e. CTRL-Return). For the other questions you need to modify the cell in order to change data set, vary the size of the network etc. There are brief comments in the code that will guide you here.\n",
    "\n",
    "From now on we will talk about *performance*! It can be performance of a trained model on the training dataset or the performance on the validation dataset. What do we mean by performance?  For classification problems we will provide 4 different measurements as returned by a call to the *stats_class* function. They are:\n",
    "* Sensitivity = fraction of correctly classified \"1\" cases\n",
    "* Specificity = fraction of correctly classified \"0\" cases\n",
    "* Accuracy = fraction of correctly classified cases\n",
    "* loss = cross-entropy error (so low loss means good performance!)\n",
    "\n",
    "Our suggestion for you is to either use accuracy or loss as your performance measure.\n",
    "\n",
    "#### Question 2\n",
    "Here you are going to train a classifier for the *syn2* dataset. You are also going to use a validation dataset as an estimate of the *true* performance. Since we generate these datasets we can allow for a relatively large validation dataset in order to get a more accurate estimation of *true* performance. The default value in the cell is to generate 1000 validation datapoints. \n",
    "\n",
    "Now, use synthetic data 2 (syn2)(100 training data points) and train a *linear* MLP to separate the two classes, i.e. use a single hidden node. **What is the performance you get on the validation dataset?** Note: Use a fixed random seed for this exercise since you will compare with runs in the next question.\n",
    "\n",
    "#### Question 3\n",
    "You are now going to overtrain the MLP! By increasing the number of hidden nodes we should be able to get better and better training performance. **How many hidden nodes do you need to reach an accuracy >95% on your training dataset?**\n",
    "\n",
    "**Hint:** Overtraining here often means finding good local minimum of the error function, which may require some tuning of the learning parameters. This means that you may have to change the learning rate, increase the number of epochs and use \"better\" minimization methods. Even though we have not yet talked about the *Adam* minimization method, it is generally better than vanilla *stochastic gradient descent*. It is therefore used in the cells below as the default minimizer. Also you may want to change the size of the \"batch_size\" parameter. It is by default using all data.\n",
    "\n",
    "#### Question 4\n",
    "The effect of overtraining can be monitored by looking at the validation performance. **(a) When you overtrained in the previous question, how much much did the validation *loss* increase, compared to the linear model of Q2?** **(b) What is the optimal number of hidden nodes for the syn2 dataset in order to maximize your validation performance?** \n",
    "\n",
    "#### Question 5\n",
    "Now you are going to use the *syn3* dataset. So, use **150** training datapoints from the synthetic dataset 3 and train an MLP to separate the two classes. Also use about 1000 datapoints for validation. **How many hidden nodes do you need to find a reasonable solution to the problem?  Extra: Can you figure out why this many?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 3\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training data\n",
    "x_train, d_train = syn2(100)\n",
    "x_val, d_val = syn2(1000)\n",
    "\n",
    "# Define the network, cost function and minimization method\n",
    "INPUT = {'inp_dim': x_train.shape[1],         \n",
    "         'n_nod': [1],                      # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'sigmoid',          # output activation function\n",
    "         'opt_method': 'Adam',               # minimization method\n",
    "         'cost_fun': 'binary_crossentropy', # error function\n",
    "         'lr_rate': 0.07 ,                    # learningrate\n",
    "         'num_out' : 1 }              # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "\n",
    "# Get the model\n",
    "model = pipline(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator = model.fit(x_train, d_train,\n",
    "                      epochs = 1000,      \n",
    "                      validation_data=(x_val, d_val),\n",
    "                      batch_size = x_train.shape[0],    # Batch size = all data (batch learning)\n",
    "                      #batch_size=50,                   # Batch size for true SGD\n",
    "                      verbose = 0)\n",
    "\n",
    "# Call the stats function to print out statistics for classification problems\n",
    "stats_class(x_train, d_train, 'Training', model)\n",
    "stats_class(x_val, d_val, 'Validation', model)\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.plot(estimator.history['loss'])\n",
    "plt.plot(estimator.history['val_loss'])\n",
    "plt.title('Model training')\n",
    "plt.ylabel('training error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "# Show the decision boundary for the training dataset\n",
    "decision_b(x_train, d_train)\n",
    "\n",
    "# If you uncomment this one you will see how the decsion boundary is with respect to the validation data\n",
    "#decision_b(x_val, d_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex3 (#10)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 6-9\n",
    "\n",
    "Now we are going to look at a regression problem. The data as described above (regr1) consists of 6 inputs (features) and one output (target) value. As for previous examples a new data set is generated each time you call the *regr1* function. To get exactly the same data set between different calls, use a fixed seed. New for this problem is that one can also control the amount of noise added to the target value. We are going to use a relatively small training dataset (~250) and a larger validation dataset (~1000) to get a more robust estimation of the generalization performance. For regression problems we also need new performance measures. The *stats_reg* function will give you two such measures:\n",
    "* MSE = mean squared error (low error mean good performance)\n",
    "* CorrCoeff = Pearson correlation coefficient for the scatter plot between predicted and true values.\n",
    "\n",
    "The cell below can be used as an template for all questions regarding this regression problem.\n",
    "\n",
    "#### Question 6\n",
    "Use 250 data points for training and about 1000 for validation and **no** added noise. Train an MLP to predict the target output. If you increase the complexity of the model (e.g. number of hidden nodes) you should be able to reach a very small training error. You will also most likely see that the validation error decreases as you increase the complexity or at least no clear sign of overtraining. **Even though the validation error is most likely still larger than the training error why do we not see any overtraining of the model? (Hint: What is it that typically causes overfitting?)**\n",
    "\n",
    "**Note:** As with previous examples you may need to tune the optimization parameters to make sure that you have \"optimal\" training. That is, increase or decrease the learningrate, possibly train longer times (increase *epochs*) and change the *batch_size* parameter.\n",
    "\n",
    "#### Question 7\n",
    "Use the same training and validation data sets as above, but add 0.4 units of noise (set the second parameter when calling the *regr1* function to 0.4 for both training and validation). Now train again, starting with a \"small\" model and increase the number of hidden nodes as you monitor the validation result for each model. **How many nodes do you have for opitimal validation performance, i.e. more hidden nodes results in overtraining?** Make a note of the validation error you obtained a this point!\n",
    "\n",
    "#### Question 8\n",
    "Instead of using the number of hidden nodes to control the complexity it is often better to use a regularization term added to the error function. You are now going to control the complexity by adding a *L2* regularizer (see the \"INPUT\" dictionary in the cell). You should modify this value until you find the \"near optimal\" validation performance. Use 15 hidden nodes. **Give the L2 value that you found to give \"optimal\" validation performance and compare with the result from  question 7 (optimal performance).**\n",
    "\n",
    "#### Question 9 \n",
    "**Summarize your findings in a few sentences.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 10\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training and validation data\n",
    "x_train, d_train = regr1(250, 0.0) # 250 data points with no noise\n",
    "x_val, d_val = regr1(1000, 0.0)\n",
    "\n",
    "# Here we need to normalize the target values\n",
    "norm_m = d_train.mean(axis=0)\n",
    "norm_s = d_train.std(axis=0)\n",
    "d_train = (d_train - norm_m) / norm_s\n",
    "\n",
    "# We use the same normalization for the validation data.\n",
    "d_val = (d_val - norm_m) / norm_s\n",
    "\n",
    "\n",
    "# Define the network, cost function and minimization method\n",
    "INPUT = {'inp_dim': x_train.shape[1],         \n",
    "         'n_nod': [3],                      # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'linear',           # output activation function\n",
    "         'opt_method': 'Adam',               # minimization method\n",
    "         'cost_fun': 'mse',                 # error function\n",
    "         'lr_rate': 0.01,                   # learningrate\n",
    "         'lambd' : 0.0,                    # L2\n",
    "         'num_out' : 1 }   # if binary --> 1 |  regression--> num output | multi-class--> num of classes\n",
    "\n",
    "# Get the model\n",
    "model = pipline(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator = model.fit(x_train, \n",
    "                      d_train,\n",
    "                      epochs = 1000,      # Number of epochs\n",
    "                      validation_data=(x_val,d_val),\n",
    "                      #batch_size = x_train.shape[0],   # Batch size = all data (batch learning)\n",
    "                      batch_size=50,                    # Batch size for true SGD\n",
    "                      verbose = 0)\n",
    "\n",
    "# Call the stats function to print out statistics for classification problems\n",
    "pred_trn = model.predict(x_train).reshape(d_train.shape)\n",
    "pred_val = model.predict(x_val).reshape(d_val.shape)\n",
    "stats_reg(d_train, pred_trn, 'Training', estimator)\n",
    "stats_reg(d_val, pred_val, 'Validation', estimator)\n",
    "\n",
    "# Scatter plots of predicted and true values\n",
    "plt.figure()\n",
    "plt.plot(d_train, pred_trn, 'g*', label='Predict vs True (Training)')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(d_val, pred_val, 'b*', label='Predict vs True (Validation)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.figure()\n",
    "plt.plot(estimator.history['loss'], label='Training')\n",
    "plt.plot(estimator.history['val_loss'], label='Validation')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex4 (#11)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 10\n",
    "\n",
    "For this exercise you are given a classification problem with a fixed training-, validation- and test dataset. The data is the Japanse vowels dataset described in the first cell. Your task is to do model selection, coming up with your optimal MLP architecture together with the hyperparameters you used. We do not provide any python code for this question, only the small part that reads the data (next code cell). There is an additional code cell that can be used to print the confusion matrix for the test data (see comments within that cell). (You can modify this function if you want to compute this matrix for both training and validation.)\n",
    "\n",
    "#### Question 10\n",
    "**Present an MLP with associated hyperparameters that maximizes the validation performance and give the test performance you obtained.**\n",
    "\n",
    "**Hint 1:** \n",
    "Remember to check if input data needs to be normalized. See the \"Ex3\" cell how this was done (in that case for the target data).\n",
    "\n",
    "**Hint 2:** \n",
    "This problem is a 9-class classification problem, meaning that you should use a specific output activation function (*out_act_fun*) and a specific loss/error function (*cost_fun*).\n",
    "\n",
    "**Hint 3:**\n",
    "The function *stats_class* function does not work here since it is designed for binary classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = vowels()\n",
    "num_classes = 9\n",
    "\n",
    "print('train input size: ',x_train.shape, 'train target size: ',y_train.shape)\n",
    "print('val   input size: ',x_val.shape, 'val   target size: ',y_val.shape)\n",
    "print('test  input size: ',x_test.shape, 'test  target size: ',y_test.shape)\n",
    "\n",
    "# Define the network, cost function and minimization method,\n",
    "# train and evaluate the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell can be used to present the classification results. \n",
    "It assumes you trained model has the name 'model_vowels'. \n",
    "It shows the test data results, but it can be changed to show\n",
    "the training/validation.\n",
    "\"\"\"\n",
    "print('\\n','#'*10,'Result for {} Data'.format('Test'), '#'*10, '\\n')\n",
    "\n",
    "y_pred = model_vowels.predict(x_test, verbose=0 )\n",
    "print('log_loss:   ', log_loss(y_test, y_pred, eps=1e-15))\n",
    "\n",
    "y_true = y_test.argmax(axis=1)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "print('accuracy:   ',(y_pred==y_true).mean(), '\\n')\n",
    "\n",
    "target_names = ['class {}'.format(i+1) for i in range(9)]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "confuTst = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm           = confuTst, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['1', '2', '3', '4', '5', '6','7', '8','9'],\n",
    "                      title        = \"Confusion Matrix: Test data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex5 (#12)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 11\n",
    "\n",
    "For this last exercise the task is to train a binary classifier for the spiral problem. The aim is to get *zero* classification error for the training data (there is no test data) with as small as possible model, in terms of the number of trainable weights. Also plot the boundary to see if it resembles a spriral. To pass this question you should at least try!\n",
    "\n",
    "#### Question 11\n",
    "**Train a classifier for the spiral problem with the aim of zero classification error with as small as possible model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "    \n",
    "# Generate training data\n",
    "x_train, d_train = spiral()\n",
    "x_train = x_train / 5\n",
    "\n",
    "# Define the network, cost function and minimization method,\n",
    "# train and evaluate the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The report!\n",
    "\n",
    "\n",
    "### Name\n",
    "\n",
    "### Introduction\n",
    "\n",
    "### Answers to questions\n",
    "\n",
    "### Summary\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Labs",
   "language": "python",
   "name": "dl-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
