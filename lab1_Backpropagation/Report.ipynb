{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0081d975-63e0-49e6-b0d7-a597288d6423",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c7c81-20d8-48f8-85ce-e25d78cf81f1",
   "metadata": {},
   "source": [
    "**Name: Erik Lidbeck** \\\n",
    "**Date: 2021-11-27** \n",
    "\n",
    "## Introduction\n",
    "This report is a summary of the first assignment in the Deep Learning course at Halmstad University. The assignment focuses on the concepts and parts of a neural network and different means to implement them. It introduces the perceptron and the multi-layer perceptron (MLP) and explains how neural networks \"learn\" with backpropagation. \n",
    "\n",
    "The first exercise explained how a neural network works \"under the hood.\" The second introduces tools that simplify implementing a neural network and explores hyperparameters' effect on a neural network.\n",
    "## Result\n",
    "### Part 1\n",
    "After implementing a two-layer MLP from scratch it proved capable of learning after 15 epochs of training. The MLP may improve by adding more layers and hidden nodes.\n",
    "    In the learning scenario, the training was done over 15 epochs, but by looking at the results, 15 epochs were redundant. It would be enough with six epochs of training judging from the graphs.\n",
    "    Increasing the learning rate may seem to increase the learning speed of the model, but this can be a stroke of luck. With a high learning rate, the model may \"miss\" the global minimum, but with a lower learning rate, it may get stuck in a local minimum and have lower accuracy. \n",
    "### Part 2\n",
    "Using PyTorch, the implementation of a neural network became quicker. When comparing the given perceptron and the first MLP (2 hidden layers, 2 hidden nodes) the perceptron performed better with lower training loss. Judging from the graphs, the perceptron had a 3e-4 loss after five epochs while the MLP(2,2) had a loss of 24e-4. Increasing the number of hidden nodes had a more significant effect on the loss than increasing the number of hidden layers. \n",
    "\n",
    "With a seemingly excessive number of hidden nodes, MLP(2, 40) got a loss of 1e-4. If the hidden layers learn features of increasing complexity, then the nodes in these layers can't be of equal \"worth\". Substituting nodes for layers decreases the knowledge the network can learn for a given feature. In this \"simple\" scenario with handwritten numbers, it doesn't make sense to have more layers, and that's why increasing the number of layers doesn't improve the performance. So no, we can't decrease the number of total nodes by exchanging nodes for layers and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7533b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
