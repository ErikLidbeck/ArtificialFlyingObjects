{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "essential-gregory",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-adaptation",
   "metadata": {},
   "source": [
    "**Name:** Erik Lidbeck\\\n",
    "**Date:** 2022-02-07\n",
    "\n",
    "## Introduction\n",
    "This lab introduces the encoder-decoder model for image segmentation where region classes are classified on a pixel scale to distinguish between objects in an image\n",
    "\n",
    "## Result\n",
    "\n",
    "### Metrics\n",
    "In this case we have a case of uneven data. The background takes up the most of the space so this is a case for accuracy vs precision vs specificity. The images consists of almost 70-90% background so if the model is able to correctly classify the background it would get an accuracy between 70-90%, but it would missclassify all other classes. So Accuracy is not a good metric.\n",
    "\n",
    "But what if we take into how big an area the classes inhabit and the area of the predicted classes? Intersection Over Union (IoU) uses the Overlap between the prediction and truth divided by the Union of the prediction and truth. This make the metric relative to the size class area. \n",
    "\n",
    "### Loss\n",
    "For loss we want something similar to IoU so the model can correct regardless of the class imbalance. A commonly used loss function is Dice loss. Dice loss is similar to IoU and you can even extract IoU from Dice with the formula: J = S / (2 + S) [J = IoU, S = Dice]. The dice coefficient is the Overlap divided by Prediction + Truth\n",
    "\n",
    "### Architecture\n",
    "#### Overfitting\n",
    "To overfit the model I tried a few methods. None of which worked. I tried to remove batch normalization which certainly made the model worse but I didn't see an increase in validation loss, only stagnation. I thought the model might've been stuck on a saddle point or plateau and increase the learning rate but it didn't make any difference. I also skipped connection multiple times thinking that more information from training would affect the model but it didn't work. I tried down and up layers thinking the larger latent space would overfit, which it didn't.\n",
    "<center><img src='./loss_overfit.png' width=\"600\"></center>\n",
    "\n",
    "I tried running for 30 epochs but noticed the models usually leveled out by 10 epochs so changed to using 10.\n",
    "\n",
    "#### Improved model\n",
    "From experimenting with overfitting I also noticed a few things that gave minimal improvements. By comparing different number of encoders and decoder with different kernel sizes I finished with 1 extra encoder-decoder. The IoU score became 0.9808 which is better than the standard that got 0.9744. I did manage to get 0.993 when i tried removing the batch normalization from the decoders, but I couldn't reproduce that result.\n",
    "\n",
    "(Gray is the improved, Blue is standard)\n",
    "<center><img src='./loss_improved.png' width=\"600\"></center>\n",
    "Gray's validation did increase to the end, but I believe tuning can improve it even further.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "For tuning I went with a \"slow and steady wins the race\" approach. In certain attempts the model's loss would spike from time to time so I thought lowering the learning rate but increasing the training epochs would improve the model.\n",
    "\n",
    "And it did. The model got a .9901 IoU on the test data. The spikes mention can be seen on the graph.\n",
    "<center><img src='./loss_tuned.png' width=\"600\"></center>\n",
    "\n",
    "### Augmentation\n",
    "For augmentation I only tested vertical flip because flipping got best improvements on the previous lab. I tested using a probability of 0.25, 0.5, 0.75 and found that 0.75 yielded best results.\n",
    "\n",
    "With 0.75 the model got 0.9901 IoU on the test data again. 0.25 and 0.5 got similar results at 0.8174 and 0.8233 respectivly.\n",
    "<center><img src='./augmented.png' width=\"600\"></center>\n",
    "I'm not entirely pleased with the architecture and believe it could do more to improve the model. Given the random results when training I have to say that the underlying data the model is trained on affects performance the most. Because of this I say think augmentation has the greatest impact on performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2dbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
