{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"font-size:40px;\">Project 1<br> Recurrent Neural Networks\n",
    "</h1></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will develop an **Recurrent neural network (RNN)**. The task is to predict the next image in a series.\n",
    "\n",
    "The task provide a dataset which creates a series of images where the last image in the series is the target. \n",
    "\n",
    "All **tasks** include **TODO's** thare are expected to be done before the deadline. The highlighted **Question's** should be answered in the report. Keep the answers separated so it is easy to read for the grading. Some sections include asserts or an expected result to give a and expected results are given. Some sections does not contain any **TODO's** but is good to understand them. \n",
    "\n",
    "For the **report** we have prepared an *Report.ipynb* notebook. The report should act as a summary of your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include all **parts** in the report!\n",
    "\n",
    "This lab logs metrics to **Tensorboard**, for instructions to install check the introduction pdf.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "**TODO:** Implement **RNN** to predict the next image in the series. \n",
    "\n",
    "**TODO:** Select a good metric to evaluate the models performance and argue why the metric give a good overview of the performance.\n",
    "\n",
    "**Question:** Explain in the report what you did and why. Present a pipeline of the code. \n",
    "\n",
    "--- \n",
    "\n",
    "**Note:** The current image resolution is set to 32x32 (i.e. IMAGE_WIDTH and IMAGE_HEIGHT) in config.py. \n",
    "This way initial experiements can run faster. Once you implement the inital version of the network, please set the resolution values back to 128x128. Experimental results should be provided for this high resolution images.  \n",
    "\n",
    "**Hint:** As a generator model, you can use the segmentation model implemented in lab03. Do not forget to adapt the input and output shapes of the generator model in this case.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Hacky solution to ac>cess the global utils package\n",
    "import sys,os\n",
    "sys.path.append(os.path.dirname(os.path.realpath('../..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import LabConfig\n",
    "import torchmetrics\n",
    "import transforms as T\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from DL_labs.utils.dataset import FutureFramePredictorDataset\n",
    "from DL_labs.utils.plot import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config\n",
    "Note that this lab does not support for fineGrained dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_img_dir': '../../data/FlyingObjectDataset_10K/training/image',\n",
       " 'validation_img_dir': '../../data/FlyingObjectDataset_10K/validation/image',\n",
       " 'testing_img_dir': '../../data/FlyingObjectDataset_10K/testing/image',\n",
       " 'SEQUENCE_LENGTH': 7,\n",
       " 'SEED': 420,\n",
       " 'GPU': -1,\n",
       " 'IMAGE_WIDTH': 128,\n",
       " 'IMAGE_HEIGHT': 128,\n",
       " 'IMAGE_CHANNEL': 3,\n",
       " 'NUM_WORKERS': 4,\n",
       " 'BATCH_SIZE': 32,\n",
       " 'TENSORBORD_DIR': 'logs/'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = LabConfig()\n",
    "cfg.todict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "First load the dataloaders for three datasets; train, validation and test. Feel free to test different augmentations, more can be found at *https://pytorch.org/vision/stable/transforms.html*\n",
    "\n",
    "The dataset consist of a series of images where the last image in the series are the target image. All other images use the target image for the prediction. \n",
    "\n",
    "Note that ToTensor and Rezise are required to reshape and transform the images correct. We do not want to apply augmentation to the test_transform that are applied on the validation and test dataloader. For semantic segmentation we do not only have an input image $x$ but also an target label $y$. If we apply example: *RandomHorizontalFlip* the expected target image will be observed but will not be same rotation.\n",
    "\n",
    "**Hint:** To ensure that the CNN take the images as a sequence the images are concatenated along the channel dimension. The input shape will look like: $(batch\\_size*sequence\\_length, C, H, W)$. Also, test: $x.view(batch\\_size, sequence\\_length, -1)$ to process the images in a sequence after CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "To ensure that each image have the same shape we pad the dataset. It is possible to resize the images but then the bounding boxes also have to be transformed to correct shape which is not covered in this project! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize((cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),interpolation=torchvision.transforms.InterpolationMode.NEAREST), \n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize((cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),interpolation=torchvision.transforms.InterpolationMode.NEAREST), #, \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch generators are created!\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    FutureFramePredictorDataset(\n",
    "        cfg.training_img_dir, \n",
    "        cfg.SEQUENCE_LENGTH,\n",
    "        img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH), \n",
    "        transforms=train_transform),\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.NUM_WORKERS\n",
    "\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    FutureFramePredictorDataset(\n",
    "        cfg.validation_img_dir, \n",
    "        cfg.SEQUENCE_LENGTH,\n",
    "        img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH), \n",
    "        transforms=test_transform),\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    FutureFramePredictorDataset(\n",
    "        cfg.testing_img_dir, \n",
    "        cfg.SEQUENCE_LENGTH,\n",
    "        img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH), \n",
    "        transforms=test_transform),\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(\"Data batch generators are created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise data\n",
    "To get an idea of the dataset we will first plot the data. This is very important, especially if we perform data augmentation. If the implementation is wrong an qualitative estimation will with a great probability tell if something is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x112 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_x, t_y = next(iter(train_dataloader))\n",
    "RNN.data(t_x, t_y, cfg.SEQUENCE_LENGTH, nrows=1, title_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Labs",
   "language": "python",
   "name": "dl-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
