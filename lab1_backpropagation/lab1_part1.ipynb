{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise I:<br> Backpropagation\n",
    "</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first lab for Deep Learning!\n",
    "\n",
    "In this lab we will scratch the surface of deep learning with backpropagation (part1) and pytorch (part2). Pytorch is a power tool for data scientists to train neural networks. Pytorch have a lot of features which can be used too train and create custom dataloaders, models and trainers in order to solve most problems related to neural networks. However, before we dive into the frameworks we will take a look at the structure of a neural network.\n",
    "\n",
    "For this lab all tasks include **TODO's** these are expected to be done before the deadline. At the end of the lab some questions shall be answered and what you have learned during the lab.\n",
    "\n",
    "There is a file called config.py. This file contains most of the settings that is used during the lab. We wont use every setting at all time but the config help us to keep organised.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy\n",
    "\n",
    "## a) Logistic Regression\n",
    "In this section we will implement a logistic regression model For the activation function we are going to focus on SGD.\n",
    "The following are going to be implemented during this lab:\n",
    "1) Logistic regression. In other words; a forward model with no hidden layers.\n",
    "2) Predict function that returns class with higest probability given an input $x$.\n",
    "3) Accuracy function that takes a batch $N$ of inputs $y_{pred}$ given an input $x$ and the expected outputs $y_{truth}$ \n",
    "4) Gradient function.\n",
    "5) Training function that uses the gradient to update $W$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: One-hot encoding\n",
    "First we need to preprocess our data. In this case we need to perform one-hot encoding on $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    \"\"\"n_classes define the length of the vector. y define the index for the 1\"\"\"\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function: Softmax functionÂ¶\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # TODO:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the softmax function works with the following cells. First we test for a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [10, 2, -3]\n",
    "softmax(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the softmax function works with the following cells. If the output from the softmax function is correct the following cell expects to sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try with two vectors which mean a batch size of $N=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([\n",
    "    [10, 2, -3],\n",
    "    [-1, 5, -20]\n",
    "])\n",
    "softmax(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we test if the softmax sum up to 1. As we have a batch of $N=2$ we expect two ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(softmax(X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "The next step is to implement the negative log likelihood loss. This algorithm is chosen to train a classification problem with C classes. The function expects the inputs; $Y_{pred}$ and $Y_{truth}$\n",
    "\n",
    "To start simple; Implement the nll loss for the probability vector and then test if the implementation can compute the average negative log likelihood of a batch where $N>1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true = np.asarray(Y_true)\n",
    "    Y_pred = np.asarray(Y_pred)\n",
    "    \n",
    "    # TODO\n",
    "    return 0.\n",
    "\n",
    "nll([1, 0, 0], [0.01, 0.01, .98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Y_pred = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "nll(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        return softmax(Z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X)) len(X.shape) == 1 else np.argmax(self.forward(X), axis=1)\n",
    "     \n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        y_pred = self.forward(x)\n",
    "        dnll_output =  y_pred - one_hot(self.output_size, y_true)\n",
    "        grad_W = np.outer(x, dnll_output)\n",
    "        grad_b = dnll_output\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update without momentum\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W = self.W - learning_rate * grads[\"W\"]\n",
    "        self.b = self.b - learning_rate * grads[\"b\"]      \n",
    "        \n",
    "    def loss(self, X, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT DONE but maybe not needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Feedforward Multilayer\n",
    "The objective of this section is to implement the backpropagation algorithm. We will focus on SGD with the chain rule on a single layer neural network with the sigmoid activation function.\n",
    "\n",
    "### Activation function\n",
    "The first task is to implement the activation function. Therefore, implement the functions; sigmoid and its element-wise derivative dsigmoid:\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    # TODO\n",
    "    return X\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    # TODO\n",
    "    return X\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO\n",
    "        self.W_h = None\n",
    "        self.b_h = None\n",
    "        self.W_o = None\n",
    "        self.b_o = None\n",
    "        self.output_size = output_size\n",
    "            \n",
    "    def forward_keep_activations(self, X):\n",
    "        # TODO\n",
    "        z_h = 0.\n",
    "        h = 0.\n",
    "        y = np.zeros(size=self.output_size)\n",
    "        return y, h, z_h\n",
    "\n",
    "    def forward(self, X):\n",
    "        y, h, z_h = self.forward_keep_activations(X)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO\n",
    "        return 42.\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO\n",
    "        return {\"W_h\": 0., \"b_h\": 0., \"W_o\": 0., \"b_o\": 0.}\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Labs",
   "language": "python",
   "name": "dl-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
