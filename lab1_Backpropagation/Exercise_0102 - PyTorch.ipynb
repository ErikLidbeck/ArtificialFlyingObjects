{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "<center><h1 style=\"font-size:40px;\">Exercise I:<br> Backpropagation\n",
    "</h1></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second part of the first lab for Deep Learning!\n",
    "\n",
    "In this lab we will scratch the surface of pytorch and deep learning. Pytorch is a power tool for data scientists to train neural networks. Pytorch have a lot of features which can be used too train and create custom dataloaders, models and trainers in order to solve most problems related to neural networks.\n",
    "\n",
    "For this lab all tasks include **TODO's** these are expected to be done before the deadline. The labs also include **Question** which should be answered and included in the *Report.ipynb*. Some sections does not contain any **TODO's** but is good to understand them. \n",
    "\n",
    "There is a file called config.py. This file contains most of the settings that is used during the lab. We wont use every setting at all time but the config help us to keep organised.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows us to edit imported files without restarting the kernel for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Hacky solution to access the global utils package\n",
    "import sys,os\n",
    "sys.path.append(os.path.dirname(os.path.realpath('..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local modules\n",
    "from config import LabConfig\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Torch packages\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LabConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a neural network model we need to implement the following class which inherits nn.Module. \n",
    "\n",
    "The first example is a single perceptron.\n",
    "\n",
    "If we assume that the trainer uses CrossEntropyLoss from pytorch we do not need to define an activation function before the output since CrossEntropyLoss combines nn.LogSoftmax() and nn.NLLLoss() according to the pytorch documentation.\n",
    "\n",
    "The input_size for this lab is the image size (cfg.IMAGE_WIDTH*cfg.IMAGE_HEIGHT) and the output size are the number of classes to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self,  input_size, output_size):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = nn.Linear(input_size,output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        output = self.fc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An architecture with 1 or more hidden layers are known as a deep/multilayer feedforward model or multilayer perceptron model (MLP). \n",
    "\n",
    "## Task 1\n",
    "\n",
    "**TODO:** Implement the MLP architecture to have 1 or more hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size,hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO\n",
    " \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "For us to load the data we will use *torchvision* transforms which allow us to pipeline the preprocessing and augmentation steps. Furthermore torchvision allow us to import the MNIST dataset and create dataloaders for batching, shuffle and multithreading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='../data', \n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='../data', \n",
    "                                        train=False, \n",
    "                                        download=True, \n",
    "                                        transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=100,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a batch from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualise the dataset we will use the function plot_grid_of_batch in order to plot the data of a batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 40\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(torchvision.utils.make_grid(trainset.data[:sample_index].unsqueeze(1).float(), normalize=True).permute(1,2,0),cmap=plt.cm.gray_r,interpolation='nearest')\n",
    "plt.title(\"MNIST example data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will learn to predict with the help of our network. The approach consist of two parts; a training and a validation step.\n",
    "\n",
    "Training consist of 4 stages:\n",
    "1. Compute model's predictions (forward pass)\n",
    "2. Compute the loss, using predictions, labels and a appropriate loss function\n",
    "3. Compute the gradients for every parameter (backpropagation) \n",
    "4. Update the parameters\n",
    "\n",
    "Validation consist of 2 stages:\n",
    "1. Compute model's predictions (forward pass)\n",
    "2. Compute the loss, using predictions, labels and a appropriate loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "**TODO:** Implement the steps to run the trainer below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, epochs):\n",
    "    loss_metric = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            # TODO: zero parameter gradients\n",
    "\n",
    "            # TODO: Feed forward\n",
    "            \n",
    "            # TODO: Calculate loss\n",
    "            \n",
    "            # TODO: Backpropagate\n",
    "     \n",
    "            # TODO: Optimizer step\n",
    "           \n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            print(f'[{epoch + 1}, {i+1:4}/{len(trainloader)}] loss: {loss.item():.4}', end='\\r')\n",
    "        loss_metric.append(loss.item()/len(trainloader))\n",
    "    return {'loss':loss_metric}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs\n",
    "For this lab we dont have a lot of configs and we keep it simple. We can define our model, number of hidden nodes, optimizer and number of epochs to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 \n",
    "**TODO:** Implement;\n",
    "1. Criterion (loss)\n",
    "2. Optimizer\n",
    "\n",
    "## Task 4\n",
    "**TODO:** \n",
    "1. Train the network with the Perceptron model.\n",
    "2. Train the MLP model and see if it performs better than the Perceptron.\n",
    "3. Extend Feedforward with additional hidden nodes until we see that the loss is going down.\n",
    "4. In this task increase the depht (more layers) of the Feedforward network instead of the size of the hidden layer.\n",
    "5. Test different hyperparameters with the MLP model. How did the result change? Is the result better or worse, why? \n",
    "\n",
    "\n",
    "**Questions:** \n",
    "1. How did the Feedforward network perform? \n",
    "2. How did the modified Feedforward network perform? \n",
    "3. Can we reduce number of nodes with an increased number of layers or vise verse, any changes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_NODES = 2\n",
    "model = Perceptron(cfg.IMAGE_WIDTH*cfg.IMAGE_HEIGHT, cfg.NUM_CLASS)# TODO\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()# TODO\n",
    "optimizer = torch.optim.Adam(model.parameters())#TODO\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "metrics = trainer(model, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the loss over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics['loss'])\n",
    "plt.title(\"Training loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets predict some data to see the performance of a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 10\n",
    "df_result = pd.DataFrame({\n",
    "    'Ground Truth': labels[:n_test],\n",
    "    'Predicted label': predicted[:n_test]})\n",
    "display(df_result.T)\n",
    "plt.imshow(torchvision.utils.make_grid(images[:n_test, :, :, :], nrow=n_test).permute(1,2,0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Labs",
   "language": "python",
   "name": "dl-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
