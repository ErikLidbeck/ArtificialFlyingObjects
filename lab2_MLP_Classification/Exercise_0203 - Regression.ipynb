{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<center><h1 style=\"font-size:40px;\">Exercise II: Regression</h1></center>\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are going to look at a regression problem. The data as described above (regr1) consists of 6 inputs (features) and one output (target) value. As for previous examples a new data set is generated each time you call the *regr1* function. To get exactly the same data set between different calls, use a fixed seed. New for this problem is that one can also control the amount of noise added to the target value. We are going to use a relatively small training dataset (\\~250) and a larger validation dataset (\\~1000) to get a more robust estimation of the generalization performance. For regression problems we also need new performance measures. The *stats_reg* function will give you two such measures:\n",
    "* MSE = mean squared error (low error mean good performance)\n",
    "* CorrCoeff = Pearson correlation coefficient for the scatter plot between predicted and true values.\n",
    "\n",
    "# Data \n",
    "## regr1\n",
    "There is also a synthetic regression problem, called *regr1*. It has 6 inputs (independent variables) and one output variable (dependent variable). It is generated according to the following formula:  \n",
    "\n",
    "$\\qquad d = 2x_1 + x_2x_3^2 + e^{x_4} + 5x_5x_6 + 3\\sin(2\\pi x_6) + \\alpha\\epsilon$  \n",
    "    \n",
    "where $\\epsilon$ is added normally distributed noise and $\\alpha$ is a parameter controlling the size of the added noise. Variables $x_1,...,x_4$ are normally distrubuted with zero mean and unit variance, whereas $x_5, x_6$ are uniformly distributed ($[0,1]$). The target value $d$ has a non-linear dependence on ***x***.\n",
    "\n",
    "\n",
    "# Tasks\n",
    "\n",
    "## Task 1\n",
    "Use 250 data points for training and about 1000 for validation and **no** added noise. Train an MLP to predict the target output. If you increase the complexity of the model (e.g. number of hidden nodes) you should be able to reach a very small training error. You will also most likely see that the validation error decreases as you increase the complexity or at least no clear sign of overtraining. \n",
    "\n",
    "**Note:** As with previous examples you may need to tune the optimization parameters to make sure that you have \"optimal\" training. That is, increase or decrease the learningrate, possibly train longer times (increase *epochs*) and change the *batch_size* parameter.\n",
    "\n",
    "**TODO:** Even though the validation error is most likely still larger than the training error why do we not see any overtraining of the model? (Hint: What is it that typically causes overfitting?)\n",
    "\n",
    "## Task 2\n",
    "Use the same training and validation data sets as above, but add 0.4 units of noise (set the second parameter when calling the *regr1* function to 0.4 for both training and validation). Now train again, starting with a \"small\" model and increase the number of hidden nodes as you monitor the validation result for each model. Make a note of the validation error you obtained a this point!\n",
    "\n",
    "**TODO:** How many nodes do you have for opitimal validation performance, i.e. more hidden nodes results in overtraining?\n",
    "\n",
    "## Task 3\n",
    "Instead of using the number of hidden nodes to control the complexity it is often better to use a regularization term added to the error function. You are now going to control the complexity by adding a *L2* regularizer (see the \"INPUT\" dictionary in the cell). You should modify this value until you find the \"near optimal\" validation performance. Use 15 hidden nodes. \n",
    "\n",
    "**TODO:** Give the L2 value that you found to give \"optimal\" validation performance and compare with the result from  question 7 (optimal performance).\n",
    "\n",
    "## Task 4\n",
    "**TODO:** Summarize your findings in a few sentences.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code allows us to edit imported files without restarting the kernel for the notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "%load_ext autoreload\r\n",
    "%autoreload 2\r\n",
    "\r\n",
    "# Hacky solution to access the global utils package\r\n",
    "import sys,os\r\n",
    "sys.path.append(os.path.dirname(os.path.realpath('')))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import pytorch_lightning as pl\r\n",
    "\r\n",
    "from config import LabConfig\r\n",
    "from dataset import MLPData\r\n",
    "from utils.model import Model\r\n",
    "from utils.progressbar import LitProgressBar\r\n",
    "from utils.model import Model\r\n",
    "from torch.utils.data import TensorDataset, DataLoader\r\n",
    "from utils import (\r\n",
    "    plot,\r\n",
    "    progressbar\r\n",
    ") "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cfg = LabConfig()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining the MLP model\n",
    "\n",
    "This cell defines the MLP model. There are a number of parameters that is needed to \n",
    "define a model. Here is a list of them: **Note:** They can all be specified when you call\n",
    "this function in later cells. The ones specified in this cell are the default values.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MLP(torch.nn.Module):\r\n",
    "    def __init__(self, \r\n",
    "                inp_dim=None,         \r\n",
    "                hidden_nodes=1, # number of nodes in hidden layer\r\n",
    "                num_out=None,\r\n",
    "                **kwargs\r\n",
    "            ):\r\n",
    "        super(MLP, self).__init__()\r\n",
    "        self.fc1 = torch.nn.Linear(inp_dim, hidden_nodes)\r\n",
    "        self.relu = torch.nn.ReLU()\r\n",
    "        self.fc2 = torch.nn.Linear(hidden_nodes, num_out)\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        hidden = self.fc1(x)\r\n",
    "        relu = self.relu(hidden)\r\n",
    "        output = self.fc2(relu)\r\n",
    "        return torch.sigmoid(output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define a function that allow us to convert numpy to pytorch DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def numpy2Dataloader(x,y, batch_size=25, num_workers=10,**kwargs):\n",
    "    return DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.from_numpy(x).float(), \n",
    "            torch.from_numpy(y).unsqueeze(1).float()\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        **kwargs\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate training and validation data\n",
    "x_train, d_train = MLPData.regr1(250, 0.0) # 250 data points with no noise\n",
    "x_val, d_val = MLPData.regr1(1000, 0.0)\n",
    "\n",
    "# Here we need to normalize the target values\n",
    "norm_m = d_train.mean(axis=0)\n",
    "norm_s = d_train.std(axis=0)\n",
    "d_train = (d_train - norm_m) / norm_s\n",
    "\n",
    "# We use the same normalization for the validation data.\n",
    "d_val = (d_val - norm_m) / norm_s\n",
    "\n",
    "train_loader = numpy2Dataloader(x_train,d_train)\n",
    "val_loader =  numpy2Dataloader(x_val,d_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "Setup our local config that should be used for the trainer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config = {\n",
    "    'max_epochs':20,\n",
    "    'model_params':{\n",
    "        'inp_dim':x_train.shape[1],         \n",
    "        'hidden_nodes':1,   # activation functions for the hidden layer\n",
    "        'num_out':1 # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "    },\n",
    "    'criterion':torch.nn.MSELoss(), # error function\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, put everything together and call on the trainers fit method. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Model(MLP(**config[\"model_params\"]),**config)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "            max_epochs=config['max_epochs'], \n",
    "            gpus=cfg.GPU,\n",
    "            logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "            callbacks=[LitProgressBar()],\n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None, # Can be None, top or full\n",
    "            num_sanity_val_steps=10,   \n",
    "        )\n",
    "trainer.fit(\n",
    "    model, \n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloaders=val_loader\n",
    ");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot.stats_class(x_train, d_train, 'Training', model)\n",
    "plot.stats_class(x_val, d_val, 'Validation', model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d_train"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('DL_labs': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "4ccf88e37874d44b4dfe33c31e1bb4a10ca4e414e0a68744582aebd290f71bcd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}